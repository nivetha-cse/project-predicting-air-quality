# -*- coding: utf-8 -*-
"""Project-Phase2 - Nivetha.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/112djd-z042HITVPTpz769yvjWPEm3ucq

upload the dataset
"""

from google .colab import files
uploaded=files.upload()

"""Load the dataset"""

import pandas as pd
df=pd.read_csv('updated_pollution_dataset.csv',sep=';')

"""Data Exploration"""

#Display first few row
df.head()

print("Shape:",df.shape)

print("Colums:, df. columns.tolist()")

df.info()

df.describe()

"""Check for Missing Values and Duplicates"""

# Check for missing values
print(df.isnull().sum())

# Check for duplicates
print("Duplicate rows:", df.duplicated().sum())

"""Visualize a Few Features"""

import matplotlib.pyplot as plt
import seaborn as sns

# Assuming 'pm2.5' is a feature in the dataset
plt.figure(figsize=(10, 6))
plt.title('Distribution of PM2.5 Levels')
plt.xlabel('PM2.5 Concentration')
plt.ylabel('Frequency')
plt.show()

import pandas as pd
import io
from google.colab import files

# Step 1: Upload the dataset
uploaded = files.upload()  # This will prompt you to upload the file
file_name = list(uploaded.keys())[0]  # Get the filename of the uploaded file

# Step 2: Read the dataset
df = pd.read_csv(io.BytesIO(uploaded[file_name])) # Use io.BytesIO to read file data

# Rest of your code...
# Step 3: Data Cleaning (if needed)
# Checking for missing values
print(df.isnull().sum())

# You may need to drop rows with missing values or fill them, e.g.:
# df = df.dropna()  # Drop rows with missing values
# or
# df.fillna(df.mean(), inplace=True)  # Fill missing values with the mean

# Step 4: Visualize Features

# Example 1: Distribution of PM2.5 levels
plt.figure(figsize=(8, 5))
sns.histplot(df['PM2.5'], kde=True, color='blue')
plt.title('Distribution of PM2.5 Levels')
plt.xlabel('PM2.5')
plt.ylabel('Frequency')
plt.show()

# Example 2: Scatter plot between PM2.5 and CO levels
plt.figure(figsize=(8, 5))
plt.title('PM2.5 vs CO Levels')
plt.xlabel('PM2.5')
plt.ylabel('CO')
plt.show()

# Example 3: Correlation heatmap between selected features
corr = df[['PM2.5', 'CO', 'Temperature', 'Humidity']].corr()
plt.figure(figsize=(8, 6))
sns.heatmap(corr, annot=True, cmap='coolwarm', fmt='.2f')
plt.title('Correlation Heatmap')
plt.show()

# Example 4: Time-series plot (if dataset contains a date column)
# Assuming there is a 'Date' column, convert it to datetime formatd


plt.figure(figsize=(10, 6))
plt.plot(df.index, df['PM2.5'], label='PM2.5', color='red')
plt.title('PM2.5 Levels Over Time')
plt.xlabel('Date')
plt.ylabel('PM2.5')
plt.xticks(rotation=45)
plt.legend()
plt.show()

"""Identify Target and Features


"""

import pandas as pd

# Load the dataset
df = pd.read_csv("updated_pollution_dataset.csv")

# Step 1: Display column names to check for actual column name
print("\n--- Available Columns in Dataset ---")
print(df.columns.tolist())

# Step 2: Define the target variable (edit this based on your dataset and output from above)
target = 'PM2.5'  # Changed to 'PM2.5' to match the actual column name

# Step 3: Check if the target exists
if target not in df.columns:
    raise ValueError(f"Target column '{target}' not found in the dataset. Available columns: {df.columns.tolist()}")

# ... (rest of your code)

"""Convert Categorical Columns to Numerical"""

import pandas as pd
from sklearn.preprocessing import LabelEncoder

# Load the dataset
df = pd.read_csv("updated_pollution_dataset.csv")

# Step 1: Check the columns and data types
print("\n--- Data Types of Columns ---")
print(df.dtypes)

# Step 2: Identify categorical columns
categorical_columns = df.select_dtypes(include=['object']).columns.tolist()
print("\n--- Categorical Columns ---")
print(categorical_columns)

# Step 3: Convert categorical columns to numerical using Label Encoding
# Label Encoding: Each category is assigned a unique integer
label_encoder = LabelEncoder()

for col in categorical_columns:
    df[col] = label_encoder.fit_transform(df[col])

# Step 4: Verify the changes
print("\n--- Data After Encoding ---")
print(df.head())

# Optional: If you want to apply One-Hot Encoding, use this approach:
# df = pd.get_dummies(df, columns=categorical_columns, drop_first=True)
# print("\n--- Data After One-Hot Encoding ---")
# print(df.head())

"""One-Hot Encoding


"""

import pandas as pd

# Load the dataset
df = pd.read_csv("updated_pollution_dataset.csv")

# Step 1: Display original column data types
print("\n--- Original Column Types ---")
print(df.dtypes)

# Step 2: Identify categorical columns (dtype = object or category)
categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()

print("\n--- Categorical Columns to Encode ---")
print(categorical_cols)

# Step 3: Apply One-Hot Encoding
# drop_first=True avoids multicollinearity (removes one dummy variable per category)
df_encoded = pd.get_dummies(df, columns=categorical_cols, drop_first=True)

# Step 4: Display the new DataFrame structure
print("\n--- Encoded DataFrame Columns ---")
print(df_encoded.columns.tolist())

print("\n--- First 5 Rows of Encoded Data ---")
print(df_encoded.head())

"""Feature Scaling"""

import pandas as pd
from sklearn.preprocessing import StandardScaler

# Step 1: Load the dataset
df = pd.read_csv("updated_pollution_dataset.csv")

# Step 2: Drop or encode categorical columns before scaling (if not already encoded)
# You should have already applied one-hot encoding at this stage
# If not, you should do that first, or exclude non-numerical columns from scaling

# Step 3: Identify numerical features
numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()

# Optional: Exclude the target variable from scaling
target = 'pm2.5'
if target in numerical_cols:
    numerical_cols.remove(target)

print("\n--- Numerical Columns to Scale ---")
print(numerical_cols)

# Step 4: Initialize the scaler
scaler = StandardScaler()

# Step 5: Apply scaling
df_scaled = df.copy()
df_scaled[numerical_cols] = scaler.fit_transform(df[numerical_cols])

# Step 6: Show scaled data
print("\n--- Scaled Feature Data ---")
print(df_scaled[numerical_cols].head())

"""Train-Test Split"""

import pandas as pd
from sklearn.model_selection import train_test_split

# Step 1: Load the dataset
df = pd.read_csv("updated_pollution_dataset.csv")

# Step 2: Define the target and feature columns
# Corrected target name to 'PM2.5' (uppercase)
target = 'PM2.5'  # Change this if your target is still different

X = df.drop(columns=[target])  # Features
y = df[target]                 # Target

# Step 3: Optional – convert categorical variables using one-hot encoding
X = pd.get_dummies(X, drop_first=True)

# Step 4: Split the dataset
# test_size=0.2 means 80% training, 20% testing
# random_state ensures reproducibility
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Step 5: Output the results
print("\n--- Train/Test Shapes ---")
print(f"X_train: {X_train.shape}")
print(f"X_test: {X_test.shape}")
print(f"y_train: {y_train.shape}")
print(f"y_test: {y_test.shape}")

"""Model Building"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, r2_score

# Step 1: Load the dataset
df = pd.read_csv("updated_pollution_dataset.csv")

# Step 2: Define target and features
# Change target to 'PM2.5' (uppercase) to match the actual column name
target = 'PM2.5'  # Replace if different
X = df.drop(columns=[target])
y = df[target]

# Step 3: Encode categorical variables (if any)
X = pd.get_dummies(X, drop_first=True)

# Step 4: Feature Scaling (recommended for regression models)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Step 5: Train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42
)

# Step 6: Initialize and train the model
model = LinearRegression()
model.fit(X_train, y_train)

# Step 7: Make predictions
y_pred = model.predict(X_test)

# Step 8: Evaluate the model
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print("\n--- Model Evaluation ---")
print(f"Mean Squared Error (MSE): {mse:.2f}")
print(f"R² Score: {r2:.2f}")

"""Evaluation"""

import numpy as np
import pandas as pd
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler


# Step 1: Load the dataset
df = pd.read_csv("updated_pollution_dataset.csv")

# Step 2: Define target and features
# Change target to 'PM2.5' (uppercase) to match the actual column name
target = 'PM2.5'  # Replace if different
X = df.drop(columns=[target])
y = df[target]

# Step 3: Encode categorical variables (if any)
X = pd.get_dummies(X, drop_first=True)

# Step 4: Feature Scaling (recommended for regression models)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Step 5: Train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42
)

# Step 6: Initialize and train the model
model = LinearRegression()
model.fit(X_train, y_train)

# Step 7: Make predictions
y_pred = model.predict(X_test)

# Evaluate performance
mae = mean_absolute_error(y_test, y_pred)

"""Make Predictions from New Input"""

import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler
import joblib  # for saving/loading model

# Step 1: Load dataset and preprocess
df = pd.read_csv("updated_pollution_dataset.csv")
target = 'PM2.5' # Changed target to 'PM2.5' to match actual column name
X = pd.get_dummies(df.drop(columns=[target]), drop_first=True)
y = df[target]

# Save column names after encoding for future use
feature_columns = X.columns

# Step 2: Scale features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Step 3: Train the model
model = LinearRegression()
model.fit(X_scaled, y)

# Optional: Save model and scaler
joblib.dump(model, 'pollution_model.pkl')
joblib.dump(scaler, 'scaler.pkl')
joblib.dump(feature_columns, 'feature_columns.pkl')

# Step 4: Predict on new input
# Example new input (must include all necessary fields as used in training)
new_data = {
    'temperature': [23],
    'humidity': [56],
    'wind_speed': [12],
    'location_Urban': [1],  # example of one-hot encoded column
    'location_Rural': [0],
    'season_Summer': [1],
    'season_Winter': [0]
}

"""Convert to DataFrame and Encode"""

import pandas as pd
import joblib

# Step 1: Load saved artifacts
feature_columns = joblib.load('feature_columns.pkl')  # previously saved during training
scaler = joblib.load('scaler.pkl')                    # for consistent scaling

# Step 2: New raw input as dictionary
new_input = {
    'temperature': [25],
    'humidity': [45],
    'wind_speed': [10],
    'location': ['Urban'],
    'season': ['Summer']
}

# Step 3: Convert to DataFrame
new_df = pd.DataFrame(new_input)

# Step 4: One-hot encode the new input
new_encoded = pd.get_dummies(new_df)

# Step 5: Align columns with training features (add missing columns with 0)
new_encoded = new_encoded.reindex(columns=feature_columns, fill_value=0)

# Step 6: Scale the input
new_scaled = scaler.transform(new_encoded)

# Optional: Use trained model to predict
model = joblib.load('pollution_model.pkl')
prediction = model.predict(new_scaled)

print("\n--- New Input Prediction ---")
print(f"Predicted PM2.5 Level: {prediction[0]:.2f}")

"""Predict the Final Grade"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestRegressor # Changed to RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score # Changed metrics for regression

# ... (rest of the code remains the same) ...

# Step 5: Train regressor
clf = RandomForestRegressor(random_state=42) # Initialize regressor
clf.fit(X_train, y_train)

# ... (update evaluation metrics to use regression metrics like MSE, R²) ...

"""Deployment-Building an Interactive App"""

!pip install streamlit
import pandas as pd
import joblib
import numpy as np
import streamlit as st # Import the streamlit library

# Step 1: Load model and scaler
# Load 'pollution_model.pkl' instead of 'final_grade_classifier.pkl'
model = joblib.load('pollution_model.pkl')
scaler = joblib.load('scaler.pkl')
feature_columns = joblib.load('feature_columns.pkl')

# Step 2: Title and instructions
st.title("Pollution Final Grade Prediction App")
st.markdown("Enter the details below to predict the final grade for pollution.")

# Step 3: Create input fields for new data
temperature = st.slider("Temperature (°C)", min_value=-10, max_value=50, value=25)
humidity = st.slider("Humidity (%)", min_value=0, max_value=100, value=50)
wind_speed = st.slider("Wind Speed (km/h)", min_value=0, max_value=50, value=10)
location = st.selectbox("Location", ['Urban', 'Rural'])
season = st.selectbox("Season", ['Spring', 'Summer', 'Fall', 'Winter'])

# Step 4: Prepare data for prediction
new_data = {
    'temperature': [temperature],
    'humidity': [humidity],
    'wind_speed': [wind_speed],
    'location': [location],
    'season': [season]
}

new_df = pd.DataFrame(new_data)

# One-hot encode the input data
new_encoded = pd.get_dummies(new_df)

# Align with training feature columns
new_encoded = new_encoded.reindex(columns=feature_columns, fill_value=0)

# Step 5: Scale the input data
new_scaled = scaler.transform(new_encoded)

# Step 6: Make prediction
prediction = model.predict(new_scaled)

# Step 7: Show prediction result
st.write(f"Predicted Final Grade: {prediction[0]}")

"""Create a Prediction Function"""

import pandas as pd
import joblib

# Load pre-trained model, scaler, and feature columns
# Changed to 'pollution_model.pkl'
model = joblib.load('pollution_model.pkl')
scaler = joblib.load('scaler.pkl')
feature_columns = joblib.load('feature_columns.pkl')

def predict_pollution_grade(new_data):
    """
    This function takes in a dictionary of input data and returns the predicted pollution grade.

    Args:
    - new_data (dict): Dictionary containing input features (e.g., temperature, humidity, etc.).

    Returns:
    - str: Predicted pollution grade (e.g., 'A', 'B', 'C', 'D').
    """
    # Step 1: Convert the dictionary into a pandas DataFrame
    new_df = pd.DataFrame(new_data)

    # Step 2: One-hot encode the input data (if categorical features exist)
    new_encoded = pd.get_dummies(new_df)

    # Step 3: Align the encoded columns with the training feature columns (add missing columns)
    new_encoded = new_encoded.reindex(columns=feature_columns, fill_value=0)

    # Step 4: Scale the input data using the same scaler used during training
    new_scaled = scaler.transform(new_encoded)

    # Step 5: Make prediction using the pre-trained model
    prediction = model.predict(new_scaled)

    # Return the prediction (final grade)
    return prediction[0]

# Example usage:
new_data = {
    'temperature': [28],  # Example temperature
    'humidity': [60],     # Example humidity
    'wind_speed': [15],   # Example wind speed
    'location': ['Urban'],  # Categorical input for location
    'season': ['Summer']    # Categorical input for season
}

predicted_grade = predict_pollution_grade(new_data)
print(f"Predicted Pollution Grade: {predicted_grade}")

"""Create the Gradio Interface"""

!pip install gradio
import pandas as pd
import joblib

# Load pre-trained model, scaler, and feature columns
model = joblib.load('pollution_model.pkl')
scaler = joblib.load('scaler.pkl')
feature_columns = joblib.load('feature_columns.pkl')

def predict_pollution_grade(temperature, humidity, wind_speed, location, season):
    """
    Function to predict pollution grade based on user input using the trained model.

    Args:
    - temperature (float): Input temperature
    - humidity (float): Input humidity
    - wind_speed (float): Input wind speed
    - location (str): Input location (Urban/Rural)
    - season (str): Input season (Spring/Summer/Fall/Winter)

    Returns:
    - str: Predicted pollution grade (A, B, C, D)
    """

    # Step 1: Create a dictionary from inputs
    new_data = {
        'temperature': [temperature],
        'humidity': [humidity],
        'wind_speed': [wind_speed],
        'location': [location],
        'season': [season]
    }

    # Step 2: Convert to DataFrame
    new_df = pd.DataFrame(new_data)

    # Step 3: One-hot encode the input data
    new_encoded = pd.get_dummies(new_df)

    # Step 4: Align the encoded columns with the training feature columns
    new_encoded = new_encoded.reindex(columns=feature_columns, fill_value=0)

    # Step 5: Scale the input data
    new_scaled = scaler.transform(new_encoded)

    # Step 6: Make prediction using the pre-trained model
    prediction = model.predict(new_scaled)

    # Return the prediction (pollution grade)
    return prediction[0]

# Step 3: Create Gradio interface
    fn=predict_pollution_grade,
    inputs=[
        gr.Slider(minimum=-10, maximum=50, step=1, label="Temperature (°C)"),
        gr.Slider(minimum=0, maximum=100, step=1, label="Humidity (%)"),
        gr.Slider(minimum=0, maximum=50, step=1, label="Wind Speed (km/h)"),
        gr.Dropdown(choices=["Urban", "Rural"], label="Location"),
        gr.Dropdown(choices=["Spring", "Summer", "Fall", "Winter"], label="Season")
    ],
    outputs="text",
!pip install gradio
import pandas as pd
import joblib
import gradio as gr

# Load pre-trained model, scaler, and feature columns
model = joblib.load('pollution_model.pkl')
scaler = joblib.load('scaler.pkl')
feature_columns = joblib.load('feature_columns.pkl')

def predict_pollution_grade(temperature, humidity, wind_speed, location, season):
    """
    Function to predict pollution grade based on user input using the trained model.

    Args:
    - temperature (float): Input temperature
    - humidity (float): Input humidity
    - wind_speed (float): Input wind speed
    - location (str): Input location (Urban/Rural)
    - season (str): Input season (Spring/Summer/Fall/Winter)

    Returns:
    - str: Predicted pollution grade (A, B, C, D)
    """

    # Step 1: Create a dictionary from inputs
    new_data = {
        'temperature': [temperature],
        'humidity': [humidity],
        'wind_speed': [wind_speed],
        'location': [location],
        'season': [season]
    }

    # Step 2: Convert to DataFrame
    new_df = pd.DataFrame(new_data)

    # Step 3: One-hot encode the input data
    new_encoded = pd.get_dummies(new_df)

    # Step 4: Align the encoded columns with the training feature columns
    new_encoded = new_encoded.reindex(columns=feature_columns, fill_value=0)

    # Step 5: Scale the input data
    new_scaled = scaler.transform(new_encoded)

    # Step 6: Make prediction using the pre-trained model
    prediction = model.predict(new_scaled)

    # Return the prediction (pollution grade)
    return prediction[0]

# Step 3: Create Gradio interface
iface = gr.Interface(
    fn=predict_pollution_grade,
    inputs=[
        gr.Slider(minimum=-10, maximum=50, step=1, label="Temperature (°C)"),
        gr.Slider(minimum=0, maximum=100, step=1, label="Humidity (%)"),
        gr.Slider(minimum=0, maximum=50, step=1, label="Wind Speed (km/h)"),
        gr.Dropdown(choices=["Urban", "Rural"], label="Location"),
        gr.Dropdown(choices=["Spring", "Summer", "Fall", "Winter"], label="Season")
    ],
    outputs="text",
    title="Pollution Final Grade Prediction",
    description="Enter values for temperature, humidity, wind speed, location, and season to predict the pollution grade."
)

# Launch the interface
iface.launch()
iface.launch()